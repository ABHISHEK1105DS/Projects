{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This is for making some large tweets to be displayed\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_csv(r'D:\\ml\\tweets-sentiment-analysis/train.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|       TV Tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>I must think about positive..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face all day! 112-102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;-------- This is the way i feel right now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man.... I'm completely useless rt now. Funny, all I can do is twitter. http://myloc.me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Feeling strangely fine. Now I'm gonna go listen to some Semisonic to celebrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>HUGE roll of thunder just now...SO scary!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>I just cut my beard off. It's only been growing for well over a year. I'm gonna start it ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>Very sad about Iran.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>You're the only one who can see this cause no one else is following me this is for you becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>&amp;lt;---Sad level is 3. I was writing a massive blog tweet on Myspace and my comp shut down. N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>...  Headed to Hospitol : Had to pull out of the Golf Tourny in 3rd place!!!!!!!!!!! I Think ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>BoRinG   ): whats wrong with him??     Please tell me........   :-/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>can't be bothered. i wish i could spend the rest of my life just sat here and going to gigs. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>Feeeling like shit right now. I really want to sleep, but nooo I have 3 hours of dancing and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>goodbye exams, HELLO ALCOHOL TONIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't realize it was THAT deep. Geez give a girl a warning atleast!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>99971</td>\n",
       "      <td>0</td>\n",
       "      <td>@CT415 @UCLA_Bruin  it made me sad too!  that means no more albums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>99972</td>\n",
       "      <td>1</td>\n",
       "      <td>@CT415 I agree. I think they all have that fetish too!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>99973</td>\n",
       "      <td>0</td>\n",
       "      <td>@ct415 I hope it's not too serious of an injury!  I'm worried!    Take a relaxing bath tonite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>99974</td>\n",
       "      <td>0</td>\n",
       "      <td>@ctabita if it's any consolation, this weekend isnt quite what i was expecting either.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>99975</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctayah got your back, yo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>99976</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctaylor0127 I can't wait to see that movie. Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>99977</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctaylor10127 @smelby I am excited and a little nervous. I can't wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>99978</td>\n",
       "      <td>0</td>\n",
       "      <td>@ctb1221 yeah  sorry.going to a concert that night.non returnable tickets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>99979</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctcash @buildingateam @diabetescure @chocolatetweet = Thanks for spreading the word for #Diabet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>99980</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctdesign87 im so glad you went to china town again  i actually think that Biryani Place's food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>99981</td>\n",
       "      <td>0</td>\n",
       "      <td>@CTerry1985  Sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>99982</td>\n",
       "      <td>0</td>\n",
       "      <td>@CTerry1985 damn it, dont have sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>99983</td>\n",
       "      <td>0</td>\n",
       "      <td>@CTerry1985 That's the thing; the new raft of Star Wars films were just a raft of #EpicFail s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>99984</td>\n",
       "      <td>1</td>\n",
       "      <td>@cthagod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>99985</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctham  #FollowFriday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>99986</td>\n",
       "      <td>0</td>\n",
       "      <td>@ctham #awaresg You are not wrong. But from a my own male point of view, I felt excluded (even w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>99987</td>\n",
       "      <td>0</td>\n",
       "      <td>@ctham @mommyfizz cuz you big burly man.  hahahahahahahahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>99988</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctham @Wilsurn Trying to get a wider range of shirts to suit everyone. Please make requests if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>99989</td>\n",
       "      <td>1</td>\n",
       "      <td>@ctham Haha I love the passion in your support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>99990</td>\n",
       "      <td>1</td>\n",
       "      <td>@cthulhullahoop That sucks...I like living in Coopersville, I don't need no special bags or anyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>99991</td>\n",
       "      <td>1</td>\n",
       "      <td>@cunningstunts till i can go home been here till saturday  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>99992</td>\n",
       "      <td>1</td>\n",
       "      <td>@cunningstunts22 afternoon jim hows you  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>99993</td>\n",
       "      <td>0</td>\n",
       "      <td>@cup_a_tea The foot is really bad. Like the worst it's ever been. I can barely walk right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>99994</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cup_Of_Katy Have fun doing health &amp;amp; safety :S Just switch off and look spritely  XXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>99995</td>\n",
       "      <td>0</td>\n",
       "      <td>@cupati It took me waaay too long to get your message about being ashamed...right now I really a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hope you're able to find something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment  \\\n",
       "0           1          0   \n",
       "1           2          0   \n",
       "2           3          1   \n",
       "3           4          0   \n",
       "4           5          0   \n",
       "5           6          0   \n",
       "6           7          1   \n",
       "7           8          0   \n",
       "8           9          1   \n",
       "9          10          1   \n",
       "10         11          0   \n",
       "11         12          1   \n",
       "12         13          0   \n",
       "13         14          0   \n",
       "14         15          0   \n",
       "15         16          0   \n",
       "16         17          0   \n",
       "17         18          1   \n",
       "18         19          0   \n",
       "19         20          0   \n",
       "20         21          0   \n",
       "21         22          0   \n",
       "22         23          1   \n",
       "23         24          0   \n",
       "24         25          0   \n",
       "25         26          0   \n",
       "26         27          0   \n",
       "27         28          0   \n",
       "28         29          1   \n",
       "29         30          0   \n",
       "...       ...        ...   \n",
       "99959   99971          0   \n",
       "99960   99972          1   \n",
       "99961   99973          0   \n",
       "99962   99974          0   \n",
       "99963   99975          1   \n",
       "99964   99976          1   \n",
       "99965   99977          1   \n",
       "99966   99978          0   \n",
       "99967   99979          1   \n",
       "99968   99980          1   \n",
       "99969   99981          0   \n",
       "99970   99982          0   \n",
       "99971   99983          0   \n",
       "99972   99984          1   \n",
       "99973   99985          1   \n",
       "99974   99986          0   \n",
       "99975   99987          0   \n",
       "99976   99988          1   \n",
       "99977   99989          1   \n",
       "99978   99990          1   \n",
       "99979   99991          1   \n",
       "99980   99992          1   \n",
       "99981   99993          0   \n",
       "99982   99994          1   \n",
       "99983   99995          0   \n",
       "99984   99996          0   \n",
       "99985   99997          1   \n",
       "99986   99998          0   \n",
       "99987   99999          1   \n",
       "99988  100000          1   \n",
       "\n",
       "                                                                                             SentimentText  \n",
       "0                                                                 is so sad for my APL friend.............  \n",
       "1                                                                         I missed the New Moon trailer...  \n",
       "2                                                                                  omg its already 7:30 :O  \n",
       "3                .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2...  \n",
       "4                                                             i think mi bf is cheating on me!!!       T_T  \n",
       "5                                                                        or i just worry too much?          \n",
       "6                                                                       Juuuuuuuuuuuuuuuuussssst Chillin!!  \n",
       "7                                                   Sunny Again        Work Tomorrow  :-|       TV Tonight  \n",
       "8                                                          handed in my uniform today . i miss you already  \n",
       "9                                                                 hmmmm.... i wonder how she my number @-)  \n",
       "10                                                                           I must think about positive..  \n",
       "11                                                 thanks to all the haters up in my face all day! 112-102  \n",
       "12                                                                          this weekend has sucked so far  \n",
       "13                                                                  jb isnt showing in australia any more!  \n",
       "14                                                                                    ok thats it you win.  \n",
       "15                                                        &lt;-------- This is the way i feel right now...  \n",
       "16         awhhe man.... I'm completely useless rt now. Funny, all I can do is twitter. http://myloc.me...  \n",
       "17                          Feeling strangely fine. Now I'm gonna go listen to some Semisonic to celebrate  \n",
       "18                                                            HUGE roll of thunder just now...SO scary!!!!  \n",
       "19         I just cut my beard off. It's only been growing for well over a year. I'm gonna start it ove...  \n",
       "20                                                                                    Very sad about Iran.  \n",
       "21                                                                                           wompppp wompp  \n",
       "22         You're the only one who can see this cause no one else is following me this is for you becau...  \n",
       "23        &lt;---Sad level is 3. I was writing a massive blog tweet on Myspace and my comp shut down. N...  \n",
       "24        ...  Headed to Hospitol : Had to pull out of the Golf Tourny in 3rd place!!!!!!!!!!! I Think ...  \n",
       "25                                     BoRinG   ): whats wrong with him??     Please tell me........   :-/  \n",
       "26        can't be bothered. i wish i could spend the rest of my life just sat here and going to gigs. ...  \n",
       "27        Feeeling like shit right now. I really want to sleep, but nooo I have 3 hours of dancing and ...  \n",
       "28                                                                   goodbye exams, HELLO ALCOHOL TONIGHT   \n",
       "29                                  I didn't realize it was THAT deep. Geez give a girl a warning atleast!  \n",
       "...                                                                                                    ...  \n",
       "99959                                  @CT415 @UCLA_Bruin  it made me sad too!  that means no more albums   \n",
       "99960                                              @CT415 I agree. I think they all have that fetish too!   \n",
       "99961       @ct415 I hope it's not too serious of an injury!  I'm worried!    Take a relaxing bath tonite.  \n",
       "99962              @ctabita if it's any consolation, this weekend isnt quite what i was expecting either.   \n",
       "99963                                                                          @ctayah got your back, yo!   \n",
       "99964                                                  @ctaylor0127 I can't wait to see that movie. Enjoy   \n",
       "99965                               @ctaylor10127 @smelby I am excited and a little nervous. I can't wait   \n",
       "99966                           @ctb1221 yeah  sorry.going to a concert that night.non returnable tickets   \n",
       "99967  @ctcash @buildingateam @diabetescure @chocolatetweet = Thanks for spreading the word for #Diabet...  \n",
       "99968  @ctdesign87 im so glad you went to china town again  i actually think that Biryani Place's food ...  \n",
       "99969                                                                                   @CTerry1985  Sorry  \n",
       "99970                                                                  @CTerry1985 damn it, dont have sky   \n",
       "99971       @CTerry1985 That's the thing; the new raft of Star Wars films were just a raft of #EpicFail s   \n",
       "99972                                                                                            @cthagod   \n",
       "99973                                                                                @ctham  #FollowFriday  \n",
       "99974  @ctham #awaresg You are not wrong. But from a my own male point of view, I felt excluded (even w...  \n",
       "99975                                         @ctham @mommyfizz cuz you big burly man.  hahahahahahahahaha  \n",
       "99976  @ctham @Wilsurn Trying to get a wider range of shirts to suit everyone. Please make requests if ...  \n",
       "99977                                                      @ctham Haha I love the passion in your support   \n",
       "99978  @cthulhullahoop That sucks...I like living in Coopersville, I don't need no special bags or anyt...  \n",
       "99979                                         @cunningstunts till i can go home been here till saturday  x  \n",
       "99980                                                           @cunningstunts22 afternoon jim hows you  x  \n",
       "99981      @cup_a_tea The foot is really bad. Like the worst it's ever been. I can barely walk right now.   \n",
       "99982            @Cup_Of_Katy Have fun doing health &amp; safety :S Just switch off and look spritely  XXX  \n",
       "99983  @cupati It took me waaay too long to get your message about being ashamed...right now I really a...  \n",
       "99984                       @Cupcake  seems like a repeating problem   hope you're able to find something.  \n",
       "99985  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll se...  \n",
       "99986                                                                       @CuPcAkE_2120 ya i thought so   \n",
       "99987                                        @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.   \n",
       "99988                                                                      @cupcake_kayla haha yes you do   \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now take a look at random tweets to gain more insights\n",
    "rand_indexs = np.random.randint(1,len(train_data),50).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84282                                                                  @chods70 you haven't said hi all day \n",
       "19340                   @ #Tesco, like my fish display?  Lemme know what you think! http://twitpic.com/4h4ga\n",
       "34321                               @ajTwist That's what im talking about! @SpekLive MANNN, Im Coolio, man. \n",
       "20092    @_TheFranchise_ I know! I miss ya!  Things are great, new SCORE material hit last week and just ...\n",
       "29487                           @acsweeney That's just not right.  i finally got to try mine and it's cold. \n",
       "52573                                                                       @Asos_Julia  need help just ask \n",
       "330                                                      #IranElection - no one realizes how deep this goes.\n",
       "10950    #Music Monday FOLLOW ME for the best UK music updates first. BEFORE NME!!  @got_andrex PLEASE I ...\n",
       "44455                                                               @Alicat13 Looks like both to me Alison. \n",
       "25365    @AdamHansen It's all about the social atmosphere. Not everybody is getting tats. Everyone is wel...\n",
       "56752               @beardoctor thanks! doc  danke? i know that's german. how do you say thank you in dutch?\n",
       "57747    @amybethbaker I'm just moving to a new apt in an area of the Chi I've ALWAYS wanted to live... a...\n",
       "59463                  @Bekei87 i tried that.   did you put just ur No. or the +61 in front?  i tried both  \n",
       "63952                                                     @BonnieBrown Well when you wake up, good morning! \n",
       "77461    @cathy_cochina40 no he's not!!! Lol u know my old boss told me the same thing about jordan too!!...\n",
       "70468                                                     @andrewjpan i cant i'm at work lah! cant tk a nap \n",
       "36121                                                 @alejandroz 140 characters or less?: Why not join it. \n",
       "28288    @AbbyArtemisia You're spot on this morning!  Back yard herbals, Dandelion syrup, Rosewater!! Tha...\n",
       "63691    @bobbyvoicu am vazut aseara salvation. mi-ar fi placut dac-as fi avut 12 ani, poate. Are FX mist...\n",
       "85120    @ccaiitllinn  I laugh when i hear they over my head cover &amp;nope  i was away, my friends did,...\n",
       "10536          #imtiredof Crying over and over again... There has to be a cure for a broken heart somewhere \n",
       "37031                                                    @aliceirene I do!!  I'm not in yo citaaay though.  \n",
       "98333                              @Cookleta tht's sweetER...  U need a *friendly punch on the shoulder* lol\n",
       "95213                        @craig88 yeah, twitterfox hadn't loaded your tweet when I said that. Congrats! \n",
       "61725    @billyraycyrus Pleaseee dad cyrus come to Puerto Rico with @mileycyrus !! and all your beautiful...\n",
       "95237                               @cinderschen Yes and I'll make some veggie juices too when you are back \n",
       "12560    &quot;black keys never looked so beautiful, and perfect rainbow never seemed so dull&quot;  *sni...\n",
       "9595           &quot;journey to the center of the earth&quot; ?! wow. that could be a huge adventure for me \n",
       "9847                                #followfriday @mck66 Best twitter friend this old hippie could ask for. \n",
       "92847                                                                                  @ChrisSlay  Have fun \n",
       "13228                 .. but for now counting down for LINES, VINES AND TRYING TIMES! what an exciting week \n",
       "53949    @AurianeD Oh your english is quit good  and I donÂ´t know if sims 3 run on my computer because i...\n",
       "15384                                 *Playing spades*...unless u can also count cards u got nothin on me.. \n",
       "46246                                       @aplusk http://twitpic.com/76riy - Oh god.. Aren't you perfect? \n",
       "85270    @chriscornell just read the interview in Guitar mag...Soundgarden reunion &quot;could happen?&qu...\n",
       "18177    @_damnprecious LMAO SORRY FOR THE DELAY. FAIL. if i were edward my absence would be much more fi...\n",
       "9659     &quot;just like the white winged dove, sings a song, sounds like she's singing. ooh, baby, ooh, ...\n",
       "21269                                              @1Marc 101 degrees + intense sun + no wind here in Texas \n",
       "69460    @bradlo yip yip you are...! now advertise guy advertise  Didn't Carmen drop off a mag for you? H...\n",
       "10788    &quot;The needs of the many outweigh the needs of the one.&quot; Aww, Spock!  He had the best fu...\n",
       "57240                           @BeckiLo Good luck with all your exams mate!! Im sure you're gonna do great \n",
       "68223        @amysav83 I feel like my brain has been gang raped.... Other than that I'm f**king marvelous!! \n",
       "3737                                                                           it stopped!! silly take40.com\n",
       "34491    @Akelaa So far, Q and Dr. McCoy have been the most vocal.  Disgusting and gross were the terms t...\n",
       "19605    @_MisterG Well Hope at your school it's not a procession of middle-class kids getting on buses a...\n",
       "60945    @bethhh_ that would be cool  i want one on my foot too,  did u know jo.bros do live chats on fac...\n",
       "66757                                                   @blusterydream I WANT TO WATCH HOUSE BUNNY SO BADLY \n",
       "15752    : local Oxfam in 'trendy' window display shocker! Cyberman helmet paired with a manequin wearing...\n",
       "84437    @choochoobear Been a few years since I went, but they still sold them last time I was through......\n",
       "12516    *sigh* Just went to fire up my new PS3 game I bought (Infamous, looks great!) and discovered tha...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"SentimentText\"][rand_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You will not have the same results at each execution because of the randomization. For me, after some execution, I noticed this:\n",
    "\n",
    "There is tweets with a url (like tweet 35546): we must think about a way to handle URLs, I thought about deleting them because a domain name or the protocol used will not make someone happy or sad unless the domain name is 'food.com'.\n",
    "The use of hashtags: we should keep only the words without '#' so words like python and the hashtag '#python' can be seen as the same word, and of course they are.\n",
    "Words like 'as', 'to' and 'so' should be deleted, because they only serve as a way to link phrases and words\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons=>make sure anlaysis to classify emoticon as happy and sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tweets_text = train_data.SentimentText.str.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{': ',\n",
       " ':$',\n",
       " \":'\",\n",
       " \":'(\",\n",
       " \":')\",\n",
       " \":'D\",\n",
       " \":'[\",\n",
       " ':(',\n",
       " ':*',\n",
       " ':-$',\n",
       " ':-*',\n",
       " ':-/',\n",
       " ':-D',\n",
       " ':-O',\n",
       " ':-P',\n",
       " ':-S',\n",
       " ':-W',\n",
       " ':-X',\n",
       " ':-[',\n",
       " ':-\\\\',\n",
       " ':-]',\n",
       " ':-h',\n",
       " ':-o',\n",
       " ':-p',\n",
       " ':-s',\n",
       " ':-x',\n",
       " ':-|',\n",
       " ':/',\n",
       " ':0',\n",
       " ':1',\n",
       " ':3',\n",
       " '::',\n",
       " ':;',\n",
       " ':?',\n",
       " ':@',\n",
       " ':C',\n",
       " ':D',\n",
       " ':E',\n",
       " ':H',\n",
       " ':I',\n",
       " ':L',\n",
       " ':O',\n",
       " ':S',\n",
       " ':T',\n",
       " ':X',\n",
       " ':Z',\n",
       " ':[',\n",
       " ':\\\\',\n",
       " ':]',\n",
       " ':b',\n",
       " ':d',\n",
       " ':l',\n",
       " ':o',\n",
       " ':p',\n",
       " ':s',\n",
       " ':x',\n",
       " ':|',\n",
       " ':}',\n",
       " ';(',\n",
       " ';)',\n",
       " ';-(',\n",
       " ';-)',\n",
       " ';-/',\n",
       " ';-;',\n",
       " ';-D',\n",
       " ';-|',\n",
       " ';.',\n",
       " ';/',\n",
       " ';3',\n",
       " ';;',\n",
       " ';D',\n",
       " ';I',\n",
       " ';P',\n",
       " ';]',\n",
       " ';d',\n",
       " ';o',\n",
       " ';p',\n",
       " ';s',\n",
       " ';t',\n",
       " 'X ',\n",
       " \"X's\",\n",
       " 'X,',\n",
       " 'X-(',\n",
       " 'X.',\n",
       " 'X1',\n",
       " 'X5',\n",
       " 'XD',\n",
       " 'XL',\n",
       " 'XM',\n",
       " 'XO',\n",
       " 'XP',\n",
       " 'XS',\n",
       " 'XT',\n",
       " 'XX',\n",
       " 'Xo',\n",
       " 'Xx',\n",
       " 'x ',\n",
       " \"x'D\",\n",
       " \"x'd\",\n",
       " 'x(',\n",
       " 'x)',\n",
       " 'x*',\n",
       " 'x,',\n",
       " 'x-',\n",
       " 'x.',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x:',\n",
       " 'x?',\n",
       " 'x@',\n",
       " 'xD',\n",
       " 'xP',\n",
       " 'xX',\n",
       " 'x]',\n",
       " 'xa',\n",
       " 'xd',\n",
       " 'xe',\n",
       " 'xh',\n",
       " 'xk',\n",
       " 'xm',\n",
       " 'xo',\n",
       " 'xp',\n",
       " 'xx',\n",
       " 'x|'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, ':/'),\n",
       " (2874, 'x '),\n",
       " (2626, ': '),\n",
       " (1339, 'x@'),\n",
       " (1214, 'xx'),\n",
       " (1162, 'xa'),\n",
       " (984, ';3'),\n",
       " (887, 'xp'),\n",
       " (842, 'xo'),\n",
       " (713, ';)'),\n",
       " (483, 'xe'),\n",
       " (431, ';I'),\n",
       " (353, ';.'),\n",
       " (254, 'xD'),\n",
       " (251, 'x.'),\n",
       " (245, '::'),\n",
       " (234, 'X '),\n",
       " (217, ';t'),\n",
       " (209, ';s'),\n",
       " (185, ':O'),\n",
       " (176, ':3'),\n",
       " (166, ';D'),\n",
       " (159, \":'\"),\n",
       " (157, 'XD'),\n",
       " (146, 'x3'),\n",
       " (142, ':p'),\n",
       " (126, \":'(\"),\n",
       " (118, ':@'),\n",
       " (117, 'xh'),\n",
       " (117, ':S'),\n",
       " (109, 'xm'),\n",
       " (104, ';p'),\n",
       " (104, ';-)'),\n",
       " (92, ':|'),\n",
       " (91, 'x,'),\n",
       " (89, ';P'),\n",
       " (76, 'xd'),\n",
       " (75, ';o'),\n",
       " (75, ';d'),\n",
       " (71, ':o'),\n",
       " (65, 'XX'),\n",
       " (63, ':L'),\n",
       " (59, 'Xx'),\n",
       " (59, ':1'),\n",
       " (58, ':]'),\n",
       " (57, ':s'),\n",
       " (56, ':0'),\n",
       " (54, 'XO'),\n",
       " (44, ';;'),\n",
       " (43, ';('),\n",
       " (38, ':-D'),\n",
       " (37, 'xk'),\n",
       " (36, 'XT'),\n",
       " (35, 'x?'),\n",
       " (35, 'x)'),\n",
       " (34, 'x2'),\n",
       " (33, ';/'),\n",
       " (32, 'x:'),\n",
       " (32, ':\\\\'),\n",
       " (31, 'x-'),\n",
       " (27, 'Xo'),\n",
       " (27, 'XP'),\n",
       " (27, ':-/'),\n",
       " (26, ':-P'),\n",
       " (25, ':*'),\n",
       " (23, 'xX'),\n",
       " (22, \":')\"),\n",
       " (17, 'xP'),\n",
       " (16, ':['),\n",
       " (16, ':-p'),\n",
       " (14, 'x]'),\n",
       " (14, 'XM'),\n",
       " (13, ':-O'),\n",
       " (12, 'x('),\n",
       " (12, 'X1'),\n",
       " (12, ':x'),\n",
       " (11, 'XS'),\n",
       " (11, ':l'),\n",
       " (10, 'x*'),\n",
       " (10, 'X.'),\n",
       " (10, ':b'),\n",
       " (10, ':T'),\n",
       " (9, ';]'),\n",
       " (9, ':I'),\n",
       " (8, ':C'),\n",
       " (7, ';-('),\n",
       " (7, ':-|'),\n",
       " (6, 'X,'),\n",
       " (6, ':-o'),\n",
       " (6, ':-\\\\'),\n",
       " (6, ':-*'),\n",
       " (6, ':$'),\n",
       " (5, 'XL'),\n",
       " (5, ':d'),\n",
       " (5, ':X'),\n",
       " (5, ':H'),\n",
       " (5, ':?'),\n",
       " (5, ':-S'),\n",
       " (4, ';-D'),\n",
       " (3, ':Z'),\n",
       " (3, ':E'),\n",
       " (3, ':-s'),\n",
       " (3, ':-['),\n",
       " (3, ':-X'),\n",
       " (2, 'X5'),\n",
       " (2, 'X-('),\n",
       " (2, \"X's\"),\n",
       " (2, ';-;'),\n",
       " (2, ':}'),\n",
       " (2, ':D'),\n",
       " (2, ':;'),\n",
       " (2, \":'D\"),\n",
       " (1, 'x|'),\n",
       " (1, \"x'd\"),\n",
       " (1, \"x'D\"),\n",
       " (1, ';-|'),\n",
       " (1, ';-/'),\n",
       " (1, ':-x'),\n",
       " (1, ':-h'),\n",
       " (1, ':-]'),\n",
       " (1, ':-W'),\n",
       " (1, ':-$'),\n",
       " (1, ':('),\n",
       " (1, \":'[\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {';D', 'xd', 'xD', ':p', ';)', ';-)', 'x)', ';-D', 'XD', ';P', ';d', ':-D', ':d', ':D', ';p'}\n",
      "Sad emoticons: {':(', \":'(\", ':|', ':/'}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most used words \n",
    "# (What we are going to do next is to define a function that will show us top words, so we may fix things before running our learning algorithm. This function takes as input a text and output words sorted according to their frequency, starting with the most used word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 133899 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'you',\n",
       " '?',\n",
       " 'a',\n",
       " 'it',\n",
       " 'i',\n",
       " '...',\n",
       " ';',\n",
       " 'and',\n",
       " '&',\n",
       " 'my',\n",
       " 'for',\n",
       " 'is',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'of',\n",
       " 'me',\n",
       " 'have',\n",
       " 'on',\n",
       " 'quot',\n",
       " \"'m\",\n",
       " 'so',\n",
       " ':',\n",
       " 'but',\n",
       " '#',\n",
       " 'do',\n",
       " 'was',\n",
       " 'be',\n",
       " 'not',\n",
       " 'your',\n",
       " 'are',\n",
       " 'just',\n",
       " 'with',\n",
       " 'like',\n",
       " '-',\n",
       " 'at',\n",
       " 'too',\n",
       " 'get',\n",
       " 'good',\n",
       " 'u',\n",
       " 'up',\n",
       " 'know',\n",
       " 'all',\n",
       " 'this',\n",
       " 'now',\n",
       " 'no',\n",
       " 'we',\n",
       " 'out',\n",
       " ')',\n",
       " 'love',\n",
       " 'can',\n",
       " '(',\n",
       " 'what',\n",
       " 'one',\n",
       " 'will',\n",
       " 'lol',\n",
       " 'go',\n",
       " 'about',\n",
       " 'did',\n",
       " \"'ll\",\n",
       " 'got',\n",
       " 'amp',\n",
       " 'there',\n",
       " 'day',\n",
       " 'http',\n",
       " 'see',\n",
       " \"'re\",\n",
       " 'if',\n",
       " 'time',\n",
       " 'they',\n",
       " 'think',\n",
       " 'as',\n",
       " 'when',\n",
       " 'from',\n",
       " 'You',\n",
       " 'It',\n",
       " 'going',\n",
       " 'really',\n",
       " 'am',\n",
       " 'work',\n",
       " 'well',\n",
       " 'had',\n",
       " 'would',\n",
       " 'how',\n",
       " 'he',\n",
       " 'here',\n",
       " 'some',\n",
       " 'thanks',\n",
       " 'back',\n",
       " 'im',\n",
       " 'haha',\n",
       " 'or']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train_data.SentimentText.str.cat())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "# (What we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 133899 different words\n"
     ]
    }
   ],
   "source": [
    "mw = most_used_words(train_data.SentimentText.str.cat())\n",
    "most_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '*hugs*',\n",
       " '*sigh*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '12',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '30SECONDSTOMARS',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'AND',\n",
       " 'Ah',\n",
       " 'AlexAllTimeLow',\n",
       " 'All',\n",
       " 'Also',\n",
       " 'Alyssa_Milano',\n",
       " 'Am',\n",
       " 'And',\n",
       " 'Are',\n",
       " 'As',\n",
       " 'At',\n",
       " 'Aw',\n",
       " 'Awesome',\n",
       " 'Aww',\n",
       " 'Awww',\n",
       " 'BSB',\n",
       " 'Birthday',\n",
       " 'But',\n",
       " 'Ca',\n",
       " 'Can',\n",
       " 'Chris',\n",
       " 'Come',\n",
       " 'Congrats',\n",
       " 'Cool',\n",
       " 'D',\n",
       " 'DM',\n",
       " 'DO',\n",
       " 'Damn',\n",
       " 'Day',\n",
       " 'Did',\n",
       " 'Do',\n",
       " 'Enjoy',\n",
       " 'FF',\n",
       " 'Follow',\n",
       " 'FollowFriday',\n",
       " 'For',\n",
       " 'Friday',\n",
       " 'Get',\n",
       " 'Glad',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Got',\n",
       " 'Great',\n",
       " 'Had',\n",
       " 'Haha',\n",
       " 'Happy',\n",
       " 'Have',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'Hope',\n",
       " 'How',\n",
       " 'I',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'If',\n",
       " 'Im',\n",
       " 'In',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Its',\n",
       " 'July',\n",
       " 'June',\n",
       " 'Just',\n",
       " 'Keep',\n",
       " 'LA',\n",
       " 'LMAO',\n",
       " 'LOL',\n",
       " 'LOVE',\n",
       " 'Let',\n",
       " 'Like',\n",
       " 'Lol',\n",
       " 'London',\n",
       " 'Love',\n",
       " 'ME',\n",
       " 'MY',\n",
       " 'Maybe',\n",
       " 'Me',\n",
       " 'Monday',\n",
       " 'Morning',\n",
       " 'My',\n",
       " 'NO',\n",
       " 'NOT',\n",
       " 'New',\n",
       " 'Nice',\n",
       " 'Night',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'O',\n",
       " 'OK',\n",
       " 'OMG',\n",
       " 'Of',\n",
       " 'Oh',\n",
       " 'Ok',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'One',\n",
       " 'Only',\n",
       " 'Or',\n",
       " 'Please',\n",
       " 'Poor',\n",
       " 'Really',\n",
       " 'S',\n",
       " 'SO',\n",
       " 'Saturday',\n",
       " 'See',\n",
       " 'She',\n",
       " 'So',\n",
       " 'Sorry',\n",
       " 'Sounds',\n",
       " 'Still',\n",
       " 'Sunday',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'TO',\n",
       " 'TV',\n",
       " 'Tell',\n",
       " 'Thank',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'They',\n",
       " 'This',\n",
       " 'To',\n",
       " 'Too',\n",
       " 'Twitter',\n",
       " 'U',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'Very',\n",
       " 'Was',\n",
       " 'We',\n",
       " 'Welcome',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Who',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'Wish',\n",
       " 'Would',\n",
       " 'Wow',\n",
       " 'XD',\n",
       " 'YAY',\n",
       " 'YES',\n",
       " 'YOU',\n",
       " 'Yay',\n",
       " 'Yeah',\n",
       " 'Yep',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'Your',\n",
       " '[',\n",
       " ']',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'account',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'ahh',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'album',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'amp',\n",
       " 'andyclemmensen',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'aplusk',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'around',\n",
       " 'ashleytisdale',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'b',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'babygirlparis',\n",
       " 'back',\n",
       " 'backstreetboys',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'bed',\n",
       " 'beer',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'big',\n",
       " 'billyraycyrus',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blog',\n",
       " 'blue',\n",
       " 'body',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'bradiewebbstack',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'btw',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'chat',\n",
       " 'check',\n",
       " 'chocolate',\n",
       " 'city',\n",
       " 'class',\n",
       " 'close',\n",
       " 'club',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'computer',\n",
       " 'concert',\n",
       " 'congrats',\n",
       " 'cool',\n",
       " 'cos',\n",
       " 'could',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'def',\n",
       " 'definitely',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'different',\n",
       " 'dinner',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'dunno',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enough',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'figure',\n",
       " 'film',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fix',\n",
       " 'flight',\n",
       " 'follow',\n",
       " 'followers',\n",
       " 'followfriday',\n",
       " 'following',\n",
       " 'food',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'front',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gon',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goodnight',\n",
       " 'gorgeous',\n",
       " 'got',\n",
       " 'great',\n",
       " 'green',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'havent',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'hehe',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hit',\n",
       " 'hmm',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'http',\n",
       " 'hug',\n",
       " 'huge',\n",
       " 'huh',\n",
       " 'hun',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'iPhone',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'idk',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'inaperfectworld',\n",
       " 'indeed',\n",
       " 'info',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'internet',\n",
       " 'invite',\n",
       " 'iphone',\n",
       " 'iremember',\n",
       " 'ive',\n",
       " 'jealous',\n",
       " 'job',\n",
       " 'join',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kid',\n",
       " 'kidding',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'lady',\n",
       " 'lame',\n",
       " 'laptop',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'life',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'lil',\n",
       " 'line',\n",
       " 'link',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'lmao',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loves',\n",
       " 'lt',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'mad',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'mate',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mention',\n",
       " 'message',\n",
       " 'met',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'move',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'moving',\n",
       " 'much',\n",
       " 'mum',\n",
       " 'music',\n",
       " 'musicmonday',\n",
       " 'must',\n",
       " 'myspace',\n",
       " 'myweakness',\n",
       " 'n',\n",
       " \"n't\",\n",
       " 'na',\n",
       " 'name',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nite',\n",
       " 'nope',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'open',\n",
       " 'order',\n",
       " 'others',\n",
       " 'outside',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'pics',\n",
       " 'picture',\n",
       " 'pictures',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'played',\n",
       " 'playing',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'pool',\n",
       " 'poor',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'power',\n",
       " 'ppl',\n",
       " 'pretty',\n",
       " 'prob',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'profile',\n",
       " 'proud',\n",
       " 'put',\n",
       " 'question',\n",
       " 'quite',\n",
       " 'quot',\n",
       " 'r',\n",
       " 'radio',\n",
       " 'rain',\n",
       " 'raining',\n",
       " 'random',\n",
       " 'rather',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'red',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'rest',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'rock',\n",
       " 'room',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'sadly',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scared',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'set',\n",
       " 'sexy',\n",
       " 'shall',\n",
       " 'shame',\n",
       " 'share',\n",
       " 'sharing',\n",
       " 'shit',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'shut',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'silly',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sis',\n",
       " 'sister',\n",
       " 'site',\n",
       " 'sitting',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smile',\n",
       " 'sold',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'son',\n",
       " 'song',\n",
       " 'songs',\n",
       " 'soo',\n",
       " 'soon',\n",
       " 'sooo',\n",
       " 'soooo',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'special',\n",
       " 'squarespace',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'stay',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'stuck',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'suck',\n",
       " 'sucks',\n",
       " 'summer',\n",
       " 'sun',\n",
       " 'sunday',\n",
       " 'sunny',\n",
       " 'super',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'sweet',\n",
       " 'sweetie',\n",
       " 'ta',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'test',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'tho',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'thx',\n",
       " 'tickets',\n",
       " 'til',\n",
       " 'till',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'top',\n",
       " 'totally',\n",
       " 'touch',\n",
       " 'tour',\n",
       " 'town',\n",
       " 'train',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'tweet',\n",
       " 'tweeting',\n",
       " 'tweets',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'type',\n",
       " 'u',\n",
       " 'ugh',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'update',\n",
       " 'updates',\n",
       " 'upset',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'version',\n",
       " 'via',\n",
       " 'video',\n",
       " 'vip',\n",
       " 'visit',\n",
       " 'voice',\n",
       " 'vote',\n",
       " 'w/',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'warm',\n",
       " 'watch',\n",
       " 'watched',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'wear',\n",
       " 'weather',\n",
       " 'website',\n",
       " 'wedding',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'win',\n",
       " 'wine',\n",
       " 'wish',\n",
       " 'wit',\n",
       " 'without',\n",
       " 'wo',\n",
       " 'woke',\n",
       " 'woman',\n",
       " 'wonder',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wow',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'wrong',\n",
       " 'x',\n",
       " 'xD',\n",
       " 'xoxo',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'ya',\n",
       " 'yay',\n",
       " 'yea',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yep',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'yo',\n",
       " 'youtube',\n",
       " 'yup',\n",
       " '|',\n",
       " '~',\n",
       " '»']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(most_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "# ( You should have noticed something, right? There are words that have the same meaning, but written in a different manner, sometimes in the plural and sometimes with a suffix (ing, es ...), this will make our model think that they are different words and also make our vocabulary bigger (waste of memory and time for the learning process). The solution is to reduce those words with the same root, this is called stemming. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# (Bag of Words)\n",
    "# We are going to use the Bag of Words algorithm, which basically takes a text as input, extract words from it (this is our vocabulary) to use them in the vectorization process. When a tweet comes in, it will vectorize it by counting the number of occurrences of each word in our vocabulary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the pipeline\n",
    "# It's always a good practice to make a pipeline of transformation for your data, it will make the process of data transformation really easy and reusable. We will implement a pipeline for transforming our tweets to something that our ML models can digest (vectors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some preprocessing of the tweets.\n",
    "# We will delete useless strings (like @, # ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            \n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "        X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "        X = X.str.lower()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline that will transform our tweets to something eatable.\n",
    "# You can see that we are using our previously defined stemmer, it will\n",
    "# take care of the stemming process.\n",
    "# For stop words, we let the inverse document frequency do the job\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentiments = train_data['Sentiment']\n",
    "tweets = train_data['SentimentText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "\n",
    "# Let's split our data into learning set and testing set\n",
    "# This process is done to test the efficency of our model at the end.\n",
    "# You shouldn't look at the test data only after choosing the final model\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
    "\n",
    "# This will tranform our learning data from simple text to vector\n",
    "# by going through the preprocessing tranformer.\n",
    "learning_data = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "models = {\n",
    "    'logitic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== logitic regression ===\n",
      "scores =  [0.81687417 0.81264271 0.81152403 0.81610027 0.81220714 0.81262089\n",
      " 0.81841247 0.80745492 0.8086136  0.8160697 ]\n",
      "mean =  0.8132519886066731\n",
      "variance =  1.15770661675637e-05\n",
      "score on the learning data (accuracy) =  0.8720710938392959\n",
      "\n",
      "=== bernoulliNB ===\n",
      "scores =  [0.79830847 0.78678609 0.78619725 0.79000234 0.78677915 0.78814054\n",
      " 0.78856213 0.78176049 0.7841988  0.78680441]\n",
      "mean =  0.7877539670192831\n",
      "variance =  1.7102828596963564e-05\n",
      "score on the learning data (accuracy) =  0.9005743513544405\n",
      "\n",
      "=== multinomialNB ===\n",
      "scores =  [0.81140058 0.8099723  0.80507061 0.80942327 0.8068859  0.81244444\n",
      " 0.81084095 0.80542584 0.80621532 0.80711506]\n",
      "mean =  0.808479427383492\n",
      "variance =  6.342225884319833e-06\n",
      "score on the learning data (accuracy) =  0.8986169847982627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models.keys():\n",
    "    scores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\n",
    "    print(\"===\", model, \"===\")\n",
    "    print(\"scores = \", scores)\n",
    "    print(\"mean = \", scores.mean())\n",
    "    print(\"variance = \", scores.var())\n",
    "    models[model].fit(learning_data, sentiments_learning)\n",
    "    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to choose the best parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the GridSearchCV does is trying different set of parameters, and for each one, it runs a cross validation and estimate the score. At the end we can see what are the best parameter and use them to build a better classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search_pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_pre_processing__use_mention': True, 'vectorizer__max_features': None, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {\n",
    "        'text_pre_processing__use_mention': [True, False],\n",
    "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
    "grid_search.fit(learn_data, sentiments_learning)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(learning_data, sentiments_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533420008667533"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = pipeline.transform(test_data)\n",
    "mnb.score(testing_data, sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ItemID  Sentiment\n",
      "0            1          0\n",
      "1            2          0\n",
      "2            3          0\n",
      "3            4          0\n",
      "4            5          0\n",
      "5            6          0\n",
      "6            7          1\n",
      "7            8          0\n",
      "8            9          0\n",
      "9           10          1\n",
      "10          11          1\n",
      "11          12          1\n",
      "12          13          0\n",
      "13          14          0\n",
      "14          15          1\n",
      "15          16          0\n",
      "16          17          1\n",
      "17          18          1\n",
      "18          19          0\n",
      "19          20          0\n",
      "20          21          0\n",
      "21          22          0\n",
      "22          23          1\n",
      "23          24          0\n",
      "24          25          1\n",
      "25          26          0\n",
      "26          27          0\n",
      "27          28          0\n",
      "28          29          1\n",
      "29          30          0\n",
      "...        ...        ...\n",
      "299959  299971          1\n",
      "299960  299972          0\n",
      "299961  299973          1\n",
      "299962  299974          0\n",
      "299963  299975          1\n",
      "299964  299976          1\n",
      "299965  299977          1\n",
      "299966  299978          1\n",
      "299967  299979          1\n",
      "299968  299980          1\n",
      "299969  299981          1\n",
      "299970  299982          1\n",
      "299971  299983          1\n",
      "299972  299984          0\n",
      "299973  299985          1\n",
      "299974  299986          0\n",
      "299975  299987          0\n",
      "299976  299988          0\n",
      "299977  299989          0\n",
      "299978  299990          1\n",
      "299979  299991          1\n",
      "299980  299992          1\n",
      "299981  299993          1\n",
      "299982  299994          1\n",
      "299983  299995          1\n",
      "299984  299996          1\n",
      "299985  299997          1\n",
      "299986  299998          1\n",
      "299987  299999          1\n",
      "299988  300000          1\n",
      "\n",
      "[299989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sub_data= pd.read_csv(r'D:\\ml\\tweets-sentiment-analysis/test.csv',encoding='ISO-8859-1')\n",
    "sub_learning = pipeline.transform(sub_data.SentimentText)\n",
    "sub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\n",
    "sub[\"Sentiment\"] = mnb.predict(sub_learning)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(learning_data, sentiments_learning)\n",
    "tweet = pd.Series([input(),])\n",
    "tweet = pipeline.transform(tweet)\n",
    "proba = model.predict_proba(tweet)[0]\n",
    "print(\"The probability that this tweet is sad is:\", proba[0])\n",
    "print(\"The probability that this tweet is happy is:\", proba[1])dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
